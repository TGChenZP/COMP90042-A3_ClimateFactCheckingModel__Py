{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 5.2 Classifier SBERT"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27437,"status":"ok","timestamp":1683340021725,"user":{"displayName":"Ron Chen","userId":"17639042157366273501"},"user_tz":-600},"id":"pBV7EEw8ws--","outputId":"470ce063-fccb-4ff0-c81a-b631290a6f4f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Read in Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IvlHUjGBxi6U"},"outputs":[],"source":["import json\n","import numpy as np\n","import pickle\n","import gc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxJxwC3uxAHJ"},"outputs":[],"source":["with open ('./drive/My Drive/LAB/COMP90042 A3/data/curated/train_claims2.json') as f:\n","    train_claims = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WnJs6bPCxdS-"},"outputs":[],"source":["with open ('./drive/My Drive/LAB/COMP90042 A3/data/raw/dev-claims.json') as f:\n","    dev_claims = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YSsQlUoC4wu_"},"outputs":[],"source":["with open ('./drive/My Drive/LAB/COMP90042 A3/data/curated/test_claims2.json') as f:\n","    test_claims = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"26hqRj0QW6rm"},"outputs":[],"source":["with open ('./drive/My Drive/LAB/COMP90042 A3/data/raw/test-claims-unlabelled.json') as f:\n","    future_claims = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3gJUbs3YxsS2"},"outputs":[],"source":["with open ('./drive/My Drive/LAB/COMP90042 A3/data/raw/evidence.json') as f:\n","    evidence = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qu3MUOzgzok0"},"outputs":[],"source":["import random\n","random.seed(19260817)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FRx-o_iL3wkT"},"outputs":[],"source":["evid_id_list = [evid_id for evid_id in evidence]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Create dataset used to train retriever"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ykXeCPAIaxI2"},"outputs":[],"source":["ENCODING = {'REFUTES': 0, 'DISPUTED': 1, 'NOT_ENOUGH_INFO': 2, 'SUPPORTS': 3}\n","DECODING = {ENCODING[key]:key for key in ENCODING}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fjkog1CQz8IN"},"outputs":[],"source":["training_data = []\n","\n","for id in train_claims:\n","\n","  claim_text = train_claims[id]['claim_text']\n","\n","  label = ENCODING[train_claims[id]['claim_label']] # LABELS MUST BE ENCODED!!!!!!\n","\n","  n_evid = len(train_claims[id]['evidences'])\n","  \n","  for evid_id in train_claims[id]['evidences']:\n","    evid_text = evidence[evid_id]\n","\n","    training_data.append(((claim_text, evid_text), label))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CBKf6VFBWSDa"},"outputs":[],"source":["dev_data = []\n","\n","for id in dev_claims:\n","\n","  claim_text = dev_claims[id]['claim_text']\n","\n","  label = ENCODING[dev_claims[id]['claim_label']] # LABELS MUST BE ENCODED!!!!!!\n","\n","  n_evid = len(dev_claims[id]['evidences'])\n","  \n","  for evid_id in dev_claims[id]['evidences']:\n","    evid_text = evidence[evid_id]\n","    \n","    dev_data.append(((claim_text, evid_text), label))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rUNy32hv43OX"},"outputs":[],"source":["test_data = []\n","\n","for id in test_claims:\n","\n","  claim_text = test_claims[id]['claim_text']\n","\n","  label = ENCODING[test_claims[id]['claim_label']] # LABELS MUST BE ENCODED!!!!!!\n","\n","  n_evid = len(test_claims[id]['evidences'])\n","  \n","  for evid_id in test_claims[id]['evidences']:\n","    evid_text = evidence[evid_id]\n","\n","    test_data.append(((claim_text, evid_text), label))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16007,"status":"ok","timestamp":1683340051588,"user":{"displayName":"Ron Chen","userId":"17639042157366273501"},"user_tz":-600},"id":"BA0eew_Zxs0O","outputId":"fb9b0221-2f99-4b3a-fd8a-ef9846c635ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1+cu118)\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"]}],"source":["!pip install torch torchvision transformers"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Build model, dataloader etc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ACUDqscx48V"},"outputs":[],"source":["from transformers import BertModel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["b2bfc68ab76046b1a122c16787981133","820200c85cf14aa1854cb64b52cf425f","6fbc130adde0487f9417e42eb74414e5","60fe8635ce0c42d2a924d1bb299bf4c1","6152c39883e4443c97a806b2ebca2bed","5808b1b38e6e4e31bd9ae1426e53653f","0c49a997090841bebc28d7b1e89e5928","526e1a49baf24d1d9ec16d8ea6a300cb","5454b7ec8961416187b414a38e31434e","4f1c0e48f450445cbad40eb92d2b84c1","a3731fc06a234a7d90193ec4fd933e7e","dfedf39d48aa4324957d70d173ba68a1","4e67c7b8023e4c33bda1f45e667eb58c","18b27938ab6d4532a1f075d22fae8a59","7e0337b9af3a4dd3ad0f07921900e442","218941ea51a24f5cacb7511294eb448b","19d7febf48dd4d6882d22b6317e8a9a3","d95b85e58e9c4138b316756edd2a7269","2490740a305b4a25b2b597fddf173f25","84641fb3a9f34fa2b21e0c494f6670d2","02155cc2e27f4320859c60ff9e28f79a","c45d8de3b3c14d2687b09e0186d6eccb"]},"executionInfo":{"elapsed":2298,"status":"ok","timestamp":1683340062296,"user":{"displayName":"Ron Chen","userId":"17639042157366273501"},"user_tz":-600},"id":"jwvoS1RjyJw3","outputId":"67b55296-b53c-47a2-e320-e3b6e49bc649"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2bfc68ab76046b1a122c16787981133","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dfedf39d48aa4324957d70d173ba68a1","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuIdphoS21tC"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from transformers import BertTokenizer\n","import pandas as pd\n","\n","class Dataset():\n","\n","    def __init__(self, data, maxlen):\n","\n","        #Store the contents of the file in a pandas dataframe\n","        self.data = data\n","\n","        #Initialize the BERT tokenizer\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","        self.maxlen = maxlen\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","\n","        #Selecting the sentence and label at the specified index in the data frame\n","        sentence1 = self.data[index][0][0]\n","        sentence2 = self.data[index][0][1]\n","        label = self.data[index][1]\n","\n","        #Preprocessing the text to be suitable for BERT\n","        tokens1 = tokenizer.tokenize(sentence1)\n","        tokens2 = tokenizer.tokenize(sentence2)\n","        tokens1 = ['[CLS]'] + tokens1 + ['[SEP]']\n","        tokens2 = ['[CLS]'] + tokens2 + ['[SEP]']\n","        if len(tokens1) < self.maxlen:\n","            tokens1 = tokens1 + ['[PAD]' for _ in range(self.maxlen - len(tokens1))] #Padding sentences\n","        else:\n","            tokens1 = tokens1[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n","\n","        if len(tokens2) < self.maxlen:\n","            tokens2 = tokens2 + ['[PAD]' for _ in range(self.maxlen - len(tokens2))] #Padding sentences\n","        else:\n","            tokens2 = tokens2[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n","\n","        tokens_ids1 = self.tokenizer.convert_tokens_to_ids(tokens1) #Obtaining the indices of the tokens in the BERT Vocabulary\n","        tokens_ids_tensor1 = torch.tensor(tokens_ids1) #Converting the list to a pytorch tensor\n","\n","        tokens_ids2 = self.tokenizer.convert_tokens_to_ids(tokens2) #Obtaining the indices of the tokens in the BERT Vocabulary\n","        tokens_ids_tensor2 = torch.tensor(tokens_ids2) #Converting the list to a pytorch tensor\n","\n","        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n","        attn_mask1 = (tokens_ids_tensor1 != 0).long()\n","        attn_mask2 = (tokens_ids_tensor2 != 0).long()\n","\n","\n","\n","        return tokens_ids_tensor1, attn_mask1, tokens_ids_tensor2, attn_mask2,  label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qvOcemHR_bHt"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","#Creating instances of training and development set\n","#maxlen sets the maximum length a sentence can have\n","#any sentence longer than this length is truncated to the maxlen size\n","train_set = Dataset(training_data, maxlen = 512)\n","dev_set = Dataset(dev_data, maxlen = 512)\n","test_set = Dataset(test_data, maxlen = 512)\n","\n","#Creating intsances of training and development dataloaders\n","train_loader = DataLoader(train_set, batch_size = 8, shuffle = True, num_workers = 2)\n","dev_loader = DataLoader(dev_set, batch_size = 8, shuffle = True, num_workers = 2)\n","test_loader = DataLoader(test_set, batch_size = 8, shuffle = True, num_workers = 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srZ-42Pz_oyO"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import BertModel\n","\n","class nEvidClassifier(nn.Module):\n","\n","    def __init__(self):\n","        super(nEvidClassifier, self).__init__()\n","        #Instantiating BERT model object \n","        self.bert_layer1 = BertModel.from_pretrained('bert-base-uncased')\n","        self.bert_layer2 = BertModel.from_pretrained('bert-base-uncased')\n","        \n","        #Classification layer\n","        #input dimension is 768 because [CLS] embedding has a dimension of 768\n","        #output dimension is 1 because we're working with a binary classification problem\n","        self.cls_layer = nn.Linear(1537, 4)\n","\n","        self.softmax_layer = nn.Softmax(dim=4)\n","\n","    def forward(self, seq1, attn_masks1, seq2, attn_masks2):\n","        '''\n","        Inputs:\n","            -seq : Tensor of shape [B, T] containing token ids of sequences\n","            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n","        '''\n","\n","        batch_size = seq1.size(0)\n","        #Feeding the input to BERT model to obtain contextualized representations\n","        claim_outputs = self.bert_layer1(seq1, attention_mask = attn_masks1, return_dict=True)\n","        claim_cont_reps = claim_outputs.last_hidden_state\n","\n","        #Obtaining the representation of [CLS] head (the first token)\n","        claim_cls_reps = claim_cont_reps[:, 0]\n","\n","        #Feeding the input to BERT model to obtain contextualized representations\n","        evid_outputs = self.bert_layer2(seq2, attention_mask = attn_masks2, return_dict=True)\n","        evid_cont_reps = evid_outputs.last_hidden_state\n","\n","        #Obtaining the representation of [CLS] head (the first token)\n","        evid_cls_reps = evid_cont_reps[:, 0]\n","\n","        # Concatenate the two output tensors along the last dimension (i.e., the features dimension)\n","        concat_output = torch.cat((claim_cls_reps, evid_cls_reps), dim=-1)\n","\n","        # Calculate the Euclidean distance between the two output tensors and flatten the result\n","        distances = []\n","        for i in range(batch_size):\n","            distance = torch.dist(claim_cls_reps[i], evid_cls_reps[i], p=2)\n","            distances.append(distance)\n","        distances = torch.flatten(torch.stack(distances)).unsqueeze(1)\n","\n","        # Concatenate the flattened distance with the concatenated output tensor\n","        concat_output = torch.cat((concat_output, distances), dim=-1)\n","\n","        #Feeding cls_rep to the classifier layer\n","        logits = self.cls_layer(concat_output)\n","\n","        return logits\n","\n","    def get_claim_embedding(self, seq1, attn_masks1):\n","        #Feeding the input to BERT model to obtain contextualized representations\n","        claim_outputs = self.bert_layer1(seq1, attention_mask = attn_masks1, return_dict=True)\n","        claim_cont_reps = claim_outputs.last_hidden_state\n","\n","        #Obtaining the representation of [CLS] head (the first token)\n","        claim_cls_reps = claim_cont_reps[:, 0]\n","\n","        return claim_cls_reps\n","    \n","\n","    def get_evid_embedding(self, seq2, attn_masks2):\n","        #Feeding the input to BERT model to obtain contextualized representations\n","        evid_outputs = self.bert_layer2(seq2, attention_mask = attn_masks2, return_dict=True)\n","        evid_cont_reps = evid_outputs.last_hidden_state\n","\n","        #Obtaining the representation of [CLS] head (the first token)\n","        evid_cls_reps = evid_cont_reps[:, 0]\n","\n","        return evid_cont_reps\n","    \n","\n","    def neural_layer(self, claim_cls_reps, evid_cls_reps):\n","\n","        # Concatenate the two output tensors along the last dimension (i.e., the features dimension)\n","        batch_size = claim_cls_reps.size(0)\n","        concat_output = torch.cat((claim_cls_reps, evid_cls_reps), dim=-1)\n","        \n","        # Calculate the Euclidean distance between the two output tensors and flatten the result\n","        distances = []\n","        for i in range(batch_size):\n","            distance = torch.dist(claim_cls_reps[i], evid_cls_reps[i], p=2)\n","            distances.append(distance)\n","        distances = torch.flatten(torch.stack(distances))\n","        \n","        # Concatenate the flattened distance with the concatenated output tensor\n","        concat_output = torch.cat((concat_output, distances), dim=-1)\n","\n","        #Feeding cls_rep to the classifier layer\n","        logits = self.cls_layer(concat_output)\n","        \n","\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4502,"status":"ok","timestamp":1683340291400,"user":{"displayName":"Ron Chen","userId":"17639042157366273501"},"user_tz":-600},"id":"fA0faPfGAeLl","outputId":"52e147bc-042c-4534-e9d7-22ad7e7df824"},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating the sentiment regressor, initialised with pretrained BERT-BASE parameters...\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Done creating the sentiment regressor.\n"]}],"source":["gpu = 0 #gpu ID\n","\n","print(\"Creating the sentiment regressor, initialised with pretrained BERT-BASE parameters...\")\n","net = nEvidClassifier()\n","net.cuda(gpu) #Enable gpu support for the model\n","print(\"Done creating the sentiment regressor.\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Setup Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGlc0FwVAlHB"},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","criterion = nn.CrossEntropyLoss()\n","opti = optim.Adam(net.parameters(), lr = 2e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L7RR7ohEAnjy"},"outputs":[],"source":["def get_accuracy_from_logits(pseudo_probs, labels):\n","\n","    correct = 0\n","    total = 0\n","    for i in range(len(labels)):\n","        _, predicted = torch.max(pseudo_probs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    return correct/total\n","\n","    # index = torch.argmax(pseudo_probs.unsqueeze(-1))\n","    # soft_probs = (probs > 0.5).long()\n","    # acc = (soft_probs.squeeze() == labels).float().mean()\n","    # return acc\n","    \n","\n","def evaluate(net, criterion, dataloader, gpu):\n","    net.eval()\n","\n","    mean_acc, mean_loss = 0, 0\n","    count = 0\n","    acc = 0\n","\n","    with torch.no_grad():\n","        for tokens_ids_tensor1, attn_mask1, tokens_ids_tensor2, attn_mask2, label in dataloader:\n","            tokens_ids_tensor1, attn_mask1, tokens_ids_tensor2, attn_mask2, label = tokens_ids_tensor1.cuda(gpu), attn_mask1.cuda(gpu), tokens_ids_tensor2.cuda(gpu), attn_mask2.cuda(gpu), label.cuda(gpu)\n","            logits = net(tokens_ids_tensor1, attn_mask1, tokens_ids_tensor2, attn_mask2)\n","            _, predicted = torch.max(logits.data, 1)\n","            acc += (predicted == label).sum().item() / len(label)\n","            mean_loss += criterion(logits, label).item()\n","            count += 1\n","\n","    return acc / count, mean_loss / count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eBze9eaUAqLg"},"outputs":[],"source":["import time\n","\n","def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n","\n","    best_acc = 0\n","    st = time.time()\n","    for ep in range(max_eps):\n","        \n","        net.train()\n","        for it, (tokens_ids_tensor1, attn_mask1, tokens_ids_tensor2, attn_mask2, label) in enumerate(train_loader):\n","            #Clear gradients\n","            opti.zero_grad()  \n","            #Converting these to cuda tensors\n","            tokens_ids_tensor1, attn_mask1, tokens_ids_tensor2, attn_mask2, label = tokens_ids_tensor1.cuda(gpu), attn_mask1.cuda(gpu), tokens_ids_tensor2.cuda(gpu), attn_mask2.cuda(gpu), label.cuda(gpu)\n","\n","            #Obtaining the logits from the model\n","            output = net(tokens_ids_tensor1, attn_mask1, tokens_ids_tensor2, attn_mask2)\n","\n","            #Computing loss\n","            loss = criterion(output, label)\n","\n","            #Backpropagating the gradients\n","            loss.backward()\n","\n","            #Optimization step\n","            opti.step()\n","              \n","            if it % 100 == 0:\n","                \n","                _, predicted = torch.max(output.data, 1)\n","                acc = (predicted == label).sum().item() / len(label)\n","                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n","                st = time.time()\n","\n","        \n","        dev_acc, dev_loss = evaluate(net, criterion, dev_loader, gpu)\n","        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n","        if dev_acc > best_acc:\n","            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n","            best_acc = dev_acc\n","            torch.save(net.state_dict(), './drive/My Drive/LAB/COMP90042 A3/models/Classifiers/Classifier_SBert.dat')\n","            torch.save(net, './drive/My Drive/LAB/COMP90042 A3/models/Classifiers/Classifier_SBert.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4luIefcfSAD"},"outputs":[],"source":["torch.cuda.empty_cache() "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3318417,"status":"ok","timestamp":1683347451300,"user":{"displayName":"Ron Chen","userId":"17639042157366273501"},"user_tz":-600},"id":"h5mhGZTSBB2S","outputId":"20784c12-15bd-408e-e93a-d04852bd1b14"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 0 of epoch 0 complete. Loss: 1.600810170173645; Accuracy: 0.125; Time taken (s): 1.5262272357940674\n","Iteration 100 of epoch 0 complete. Loss: 1.572058916091919; Accuracy: 0.0; Time taken (s): 138.86718797683716\n","Iteration 200 of epoch 0 complete. Loss: 1.5358103513717651; Accuracy: 0.25; Time taken (s): 139.95444679260254\n","Iteration 300 of epoch 0 complete. Loss: 1.3779265880584717; Accuracy: 0.25; Time taken (s): 139.89934015274048\n","Iteration 400 of epoch 0 complete. Loss: 1.4160341024398804; Accuracy: 0.25; Time taken (s): 140.04744601249695\n","Epoch 0 complete! Development Accuracy: 0.22311827956989247; Development Loss: 1.449906587600708\n","Best development accuracy improved from 0 to 0.22311827956989247, saving model...\n","Iteration 0 of epoch 1 complete. Loss: 1.4441471099853516; Accuracy: 0.125; Time taken (s): 109.96489429473877\n","Iteration 100 of epoch 1 complete. Loss: 1.4494788646697998; Accuracy: 0.125; Time taken (s): 140.17534637451172\n","Iteration 200 of epoch 1 complete. Loss: 1.5565779209136963; Accuracy: 0.0; Time taken (s): 139.9172191619873\n","Iteration 300 of epoch 1 complete. Loss: 1.5760892629623413; Accuracy: 0.0; Time taken (s): 139.793438911438\n","Iteration 400 of epoch 1 complete. Loss: 1.4503191709518433; Accuracy: 0.375; Time taken (s): 139.9747438430786\n","Epoch 1 complete! Development Accuracy: 0.22311827956989247; Development Loss: 1.4524000479329018\n","Iteration 0 of epoch 2 complete. Loss: 1.4560184478759766; Accuracy: 0.125; Time taken (s): 102.70177984237671\n","Iteration 100 of epoch 2 complete. Loss: 1.3829196691513062; Accuracy: 0.25; Time taken (s): 140.01050424575806\n","Iteration 200 of epoch 2 complete. Loss: 1.5377390384674072; Accuracy: 0.125; Time taken (s): 139.58802890777588\n","Iteration 300 of epoch 2 complete. Loss: 1.509117603302002; Accuracy: 0.125; Time taken (s): 139.89557790756226\n","Iteration 400 of epoch 2 complete. Loss: 1.5450234413146973; Accuracy: 0.0; Time taken (s): 139.71463656425476\n","Epoch 2 complete! Development Accuracy: 0.22311827956989247; Development Loss: 1.450760006904602\n","Iteration 0 of epoch 3 complete. Loss: 1.498570203781128; Accuracy: 0.125; Time taken (s): 102.60140323638916\n","Iteration 100 of epoch 3 complete. Loss: 1.4814116954803467; Accuracy: 0.25; Time taken (s): 139.7139995098114\n","Iteration 200 of epoch 3 complete. Loss: 1.4917224645614624; Accuracy: 0.125; Time taken (s): 140.0215299129486\n","Iteration 300 of epoch 3 complete. Loss: 1.4730499982833862; Accuracy: 0.25; Time taken (s): 139.81981301307678\n","Iteration 400 of epoch 3 complete. Loss: 1.4618401527404785; Accuracy: 0.25; Time taken (s): 139.92004466056824\n","Epoch 3 complete! Development Accuracy: 0.21975806451612903; Development Loss: 1.4539124619576238\n","Iteration 0 of epoch 4 complete. Loss: 1.524269700050354; Accuracy: 0.375; Time taken (s): 102.84122204780579\n","Iteration 100 of epoch 4 complete. Loss: 1.5151383876800537; Accuracy: 0.125; Time taken (s): 139.7655577659607\n","Iteration 200 of epoch 4 complete. Loss: 1.6078665256500244; Accuracy: 0.125; Time taken (s): 139.90575504302979\n","Iteration 300 of epoch 4 complete. Loss: 1.3939332962036133; Accuracy: 0.25; Time taken (s): 139.8002314567566\n","Iteration 400 of epoch 4 complete. Loss: 1.3820210695266724; Accuracy: 0.25; Time taken (s): 139.82688450813293\n","Epoch 4 complete! Development Accuracy: 0.22311827956989247; Development Loss: 1.4503885699856667\n"]}],"source":["num_epoch = 5\n","\n","#fine-tune the model\n","train(net, criterion, opti, train_loader, dev_loader, num_epoch, gpu)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Read In Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nI4Ba8fl_fX9"},"outputs":[],"source":["net = torch.load('./drive/My Drive/LAB/COMP90042 A3/models/temporary_models/StateClassifier_sentence.pt')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hAr7gsRG_uJi"},"source":["## Make Predictions (make Classifications)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"igwVNDPz_qfJ"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from transformers import BertTokenizer\n","import pandas as pd\n","\n","class PredictDataset():\n","\n","    def __init__(self, data, maxlen):\n","\n","        #Store the contents of the file in a pandas dataframe\n","        self.data = data\n","\n","        #Initialize the BERT tokenizer\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","        self.maxlen = maxlen\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","\n","        #Selecting the sentence and label at the specified index in the data frame\n","        sentence1 = self.data[index][0][0]\n","        sentence2 = self.data[index][0][1]\n","\n","        #Preprocessing the text to be suitable for BERT\n","        tokens1 = tokenizer.tokenize(sentence1)\n","        tokens2 = tokenizer.tokenize(sentence2)\n","        tokens1 = ['[CLS]'] + tokens1 + ['[SEP]']\n","        tokens2 = ['[CLS]'] + tokens2 + ['[SEP]']\n","        if len(tokens1) < self.maxlen:\n","            tokens1 = tokens1 + ['[PAD]' for _ in range(self.maxlen - len(tokens1))] #Padding sentences\n","        else:\n","            tokens1 = tokens1[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n","\n","        if len(tokens2) < self.maxlen:\n","            tokens2 = tokens2 + ['[PAD]' for _ in range(self.maxlen - len(tokens2))] #Padding sentences\n","        else:\n","            tokens2 = tokens2[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n","\n","        tokens_ids1 = self.tokenizer.convert_tokens_to_ids(tokens1) #Obtaining the indices of the tokens in the BERT Vocabulary\n","        tokens_ids_tensor1 = torch.tensor(tokens_ids1) #Converting the list to a pytorch tensor\n","\n","        tokens_ids2 = self.tokenizer.convert_tokens_to_ids(tokens2) #Obtaining the indices of the tokens in the BERT Vocabulary\n","        tokens_ids_tensor2 = torch.tensor(tokens_ids2) #Converting the list to a pytorch tensor\n","\n","        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n","        attn_mask1 = (tokens_ids_tensor1 != 0).long()\n","        attn_mask2 = (tokens_ids_tensor2 != 0).long()\n","\n","\n","\n","        return tokens_ids_tensor1, attn_mask1, tokens_ids_tensor2, attn_mask2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":104767,"status":"ok","timestamp":1683348220896,"user":{"displayName":"Ron Chen","userId":"05494325491966814875"},"user_tz":-600},"id":"Y6eEox0gAV4L","outputId":"f9d24944-2412-4c66-ef1c-1689c66a1f85"},"outputs":[{"name":"stdout","output_type":"stream","text":["claim-752\n","claim-375\n","claim-1266\n","claim-871\n","claim-2164\n","claim-1607\n","claim-761\n","claim-1718\n","claim-1273\n","claim-1786\n","claim-2796\n","claim-2580\n","claim-1219\n","claim-75\n","claim-2813\n","claim-2335\n","claim-161\n","claim-2243\n","claim-1256\n","claim-506\n","claim-369\n","claim-2184\n","claim-1057\n","claim-104\n","claim-1975\n","claim-139\n","claim-2062\n","claim-1160\n","claim-2679\n","claim-2662\n","claim-1490\n","claim-2768\n","claim-2168\n","claim-785\n","claim-2426\n","claim-1292\n","claim-993\n","claim-2593\n","claim-1567\n","claim-1834\n","claim-856\n","claim-540\n","claim-757\n","claim-1407\n","claim-3070\n","claim-1745\n","claim-1515\n","claim-1519\n","claim-3069\n","claim-677\n","claim-765\n","claim-2275\n","claim-1113\n","claim-2611\n","claim-2060\n","claim-2326\n","claim-1087\n","claim-2867\n","claim-2300\n","claim-2250\n","claim-2429\n","claim-3051\n","claim-1549\n","claim-261\n","claim-2230\n","claim-2579\n","claim-1416\n","claim-2497\n","claim-811\n","claim-1896\n","claim-2819\n","claim-2643\n","claim-1775\n","claim-316\n","claim-896\n","claim-331\n","claim-2574\n","claim-342\n","claim-2034\n","claim-578\n","claim-976\n","claim-1097\n","claim-609\n","claim-173\n","claim-1222\n","claim-2441\n","claim-756\n","claim-2577\n","claim-2890\n","claim-2478\n","claim-2399\n","claim-3091\n","claim-141\n","claim-1933\n","claim-1689\n","claim-443\n","claim-2037\n","claim-1734\n","claim-2093\n","claim-1400\n","claim-1638\n","claim-3075\n","claim-38\n","claim-1643\n","claim-1259\n","claim-1605\n","claim-1711\n","claim-2236\n","claim-1040\n","claim-392\n","claim-368\n","claim-559\n","claim-2583\n","claim-2609\n","claim-492\n","claim-1420\n","claim-1089\n","claim-1467\n","claim-444\n","claim-803\n","claim-1668\n","claim-742\n","claim-846\n","claim-2119\n","claim-1167\n","claim-623\n","claim-2882\n","claim-1698\n","claim-181\n","claim-281\n","claim-2809\n","claim-1928\n","claim-2787\n","claim-478\n","claim-988\n","claim-266\n","claim-2282\n","claim-2895\n","claim-349\n","claim-2101\n","claim-897\n","claim-3063\n","claim-386\n","claim-2691\n","claim-530\n","claim-2979\n","claim-665\n","claim-199\n","claim-490\n","claim-2400\n","claim-204\n","claim-1426\n","claim-698\n","claim-1021\n"]}],"source":["\n","def get_classif(claims, file_name, SIZE):\n","\n","\n","  try:\n","    with open(f'./drive/My Drive/LAB/COMP90042 A3/predictions/Retrievals/{file_name}.pickle', 'rb') as f:\n","      predictions = pickle.load(f)\n","  except:\n","      predictions = {}\n","\n","\n","  i = 0\n","  for id in claims:\n","    if id in predictions:\n","      print('pass:', id)\n","      continue\n","\n","    print(id)\n","\n","    claim_text = claims[id]['claim_text']\n","    \n","    data_for_predict = list()\n","\n","    for evid_id in claims[id]['evidences']:\n","      evid_text = evidence[evid_id]\n","      \n","      data_for_predict.append((claim_text, evid_text))\n","\n","    set_for_predict = PredictDataset(data_for_predict, maxlen = 512)\n","\n","    predict_loader = DataLoader(set_for_predict, batch_size = SIZE, num_workers = 2)\n","    \n","    predicted_logit = list()\n","    with torch.no_grad():\n","      for it, (seq, attn_masks, seg_ids_tensor) in enumerate(predict_loader):\n","        \n","        torch.cuda.empty_cache()\n","        gc.collect()\n","        seq, attn_masks, seg_ids_tensor = seq.cuda(gpu), attn_masks.cuda(gpu), seg_ids_tensor.cuda(gpu)\n","\n","        logits = net(seq, attn_masks, seg_ids_tensor).tolist()\n","      \n","      predictions[id] = logits\n","\n","      with open(f'./drive/My Drive/LAB/COMP90042 A3/predictions/Retrievals/{file_name}.pickle', 'wb') as f:\n","          pickle.dump(predictions, f)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Classifications from Ground Truth"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjA24LYuMf0w"},"outputs":[],"source":["get_retrievals(dev_claims, 'Classification_BERT_dev', SIZE=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NsoiYxAGMqvc"},"outputs":[],"source":["get_retrievals(test_claims, 'Classification_BERT_test', SIZE=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uB8RzpVpdGl3"},"outputs":[],"source":["with open('./drive/My Drive/LAB/COMP90042 A3/models/predictions/Retrievals/Retrieval_BERT_redued_dev.pickle', 'rb') as f:\n","    dev_predicted_evidence = pickle.load(f)\n","\n","dev_claims_pred = {}\n","\n","for claim in dev_predicted_evidence:\n","  evid_list = [x[0] for x in dev_predicted_evidence[claim]['evidences']]\n","\n","  dev_claims_pred[claim] = evid_list[:4]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGGjVioCdKIL"},"outputs":[],"source":["with open('./drive/My Drive/LAB/COMP90042 A3/models/predictions/Retrievals/Retrieval_BERT_redued_test.pickle', 'rb') as f:\n","    test_predicted_evidence = pickle.load(f)\n","\n","test_claims_pred = {}\n","\n","for claim in test_predicted_evidence:\n","  evid_list = [x[0] for x in test_predicted_evidence[claim]['evidences']]\n","\n","  test_claims_pred[claim] = evid_list[:4]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gl3Gig9qdMiE"},"outputs":[],"source":["with open('./drive/My Drive/LAB/COMP90042 A3/models/predictions/Retrievals/Retrieval_BERT_redued_future.pickle', 'rb') as f:\n","    future_predicted_evidence = pickle.load(f)\n","\n","future_claims_pred = {}\n","\n","for claim in future_predicted_evidence:\n","  evid_list = [x[0] for x in future_predicted_evidence[claim]['evidences']]\n","\n","  future_claims_pred[claim] = evid_list[:4]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Classifications from Predicted Evidence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U0SP0_uMdmtT"},"outputs":[],"source":["get_retrievals(future_claims_dev, 'Classification_BERT_devp-bert', SIZE=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Fys1DpRdl_i"},"outputs":[],"source":["get_retrievals(future_claims_test, 'Classification_BERT_testp-bert', SIZE=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eIZfSYjEMj6u"},"outputs":[],"source":["get_retrievals(future_claims_pred, 'Classification_BERT_futurep-bert', SIZE=32)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"02155cc2e27f4320859c60ff9e28f79a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c49a997090841bebc28d7b1e89e5928":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"111a16414788496d80ad188fc9659d0c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63e6ea9a12f64b50b60a5ddaebbd8aa6","placeholder":"​","style":"IPY_MODEL_d8596a4911ba443b9f7e07c3b870fe38","value":"Downloading (…)lve/main/config.json: 100%"}},"17b510c4806d491695dbf655fcdd49c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_56675a6e8bf5454a97d018923f501468","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e16b6b60d26d47dfb126fd55017b79f9","value":440473133}},"18b27938ab6d4532a1f075d22fae8a59":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2490740a305b4a25b2b597fddf173f25","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84641fb3a9f34fa2b21e0c494f6670d2","value":28}},"19d7febf48dd4d6882d22b6317e8a9a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"218941ea51a24f5cacb7511294eb448b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2490740a305b4a25b2b597fddf173f25":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cc8a0e46ff84d0e94ccf50bdd2b76b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_72d7a1dc85ac4a1a935c4fe2eca50579","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_62de9ff551534a02acf0de3a8c328ff9","value":570}},"48d47e401d09473e87b1b4579801de66":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bf317f30b9c4573a2c3c9bd970dc6b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e67c7b8023e4c33bda1f45e667eb58c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19d7febf48dd4d6882d22b6317e8a9a3","placeholder":"​","style":"IPY_MODEL_d95b85e58e9c4138b316756edd2a7269","value":"Downloading (…)okenizer_config.json: 100%"}},"4f1c0e48f450445cbad40eb92d2b84c1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"526e1a49baf24d1d9ec16d8ea6a300cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5454b7ec8961416187b414a38e31434e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"56675a6e8bf5454a97d018923f501468":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5808b1b38e6e4e31bd9ae1426e53653f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58da66af68d64ea0bfca7240c9b2b78f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60fe8635ce0c42d2a924d1bb299bf4c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f1c0e48f450445cbad40eb92d2b84c1","placeholder":"​","style":"IPY_MODEL_a3731fc06a234a7d90193ec4fd933e7e","value":" 232k/232k [00:00&lt;00:00, 546kB/s]"}},"6152c39883e4443c97a806b2ebca2bed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62de9ff551534a02acf0de3a8c328ff9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"63e6ea9a12f64b50b60a5ddaebbd8aa6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b1d8cb32e0f4e5798c97cad8ccd9651":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fbc130adde0487f9417e42eb74414e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_526e1a49baf24d1d9ec16d8ea6a300cb","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5454b7ec8961416187b414a38e31434e","value":231508}},"72d7a1dc85ac4a1a935c4fe2eca50579":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7631152ae2e449e782e891758d99dc70":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e0337b9af3a4dd3ad0f07921900e442":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02155cc2e27f4320859c60ff9e28f79a","placeholder":"​","style":"IPY_MODEL_c45d8de3b3c14d2687b09e0186d6eccb","value":" 28.0/28.0 [00:00&lt;00:00, 1.25kB/s]"}},"820200c85cf14aa1854cb64b52cf425f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5808b1b38e6e4e31bd9ae1426e53653f","placeholder":"​","style":"IPY_MODEL_0c49a997090841bebc28d7b1e89e5928","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"84641fb3a9f34fa2b21e0c494f6670d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9274e86426a64406b296993363576c3b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58da66af68d64ea0bfca7240c9b2b78f","placeholder":"​","style":"IPY_MODEL_6b1d8cb32e0f4e5798c97cad8ccd9651","value":"Downloading pytorch_model.bin: 100%"}},"a30db147dd96411994d3b68e5bc1eae1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a42e4c26b89e463485588c2f4adbf385","placeholder":"​","style":"IPY_MODEL_cce4ed176f5f49259c7f8056b0c3d607","value":" 440M/440M [00:01&lt;00:00, 324MB/s]"}},"a3731fc06a234a7d90193ec4fd933e7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a42e4c26b89e463485588c2f4adbf385":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2bfc68ab76046b1a122c16787981133":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_820200c85cf14aa1854cb64b52cf425f","IPY_MODEL_6fbc130adde0487f9417e42eb74414e5","IPY_MODEL_60fe8635ce0c42d2a924d1bb299bf4c1"],"layout":"IPY_MODEL_6152c39883e4443c97a806b2ebca2bed"}},"b3307615a3f3432495b1e0324b090f51":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcc8ad1b25cb4c14893027550a60161e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_111a16414788496d80ad188fc9659d0c","IPY_MODEL_2cc8a0e46ff84d0e94ccf50bdd2b76b7","IPY_MODEL_ce2d954a6cce4d3abd4d70596fbf9f76"],"layout":"IPY_MODEL_b3307615a3f3432495b1e0324b090f51"}},"c45d8de3b3c14d2687b09e0186d6eccb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cce4ed176f5f49259c7f8056b0c3d607":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce2d954a6cce4d3abd4d70596fbf9f76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48d47e401d09473e87b1b4579801de66","placeholder":"​","style":"IPY_MODEL_4bf317f30b9c4573a2c3c9bd970dc6b7","value":" 570/570 [00:00&lt;00:00, 30.1kB/s]"}},"d8050f74f8e646e68e672d59fd08b10d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9274e86426a64406b296993363576c3b","IPY_MODEL_17b510c4806d491695dbf655fcdd49c2","IPY_MODEL_a30db147dd96411994d3b68e5bc1eae1"],"layout":"IPY_MODEL_7631152ae2e449e782e891758d99dc70"}},"d8596a4911ba443b9f7e07c3b870fe38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d95b85e58e9c4138b316756edd2a7269":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dfedf39d48aa4324957d70d173ba68a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4e67c7b8023e4c33bda1f45e667eb58c","IPY_MODEL_18b27938ab6d4532a1f075d22fae8a59","IPY_MODEL_7e0337b9af3a4dd3ad0f07921900e442"],"layout":"IPY_MODEL_218941ea51a24f5cacb7511294eb448b"}},"e16b6b60d26d47dfb126fd55017b79f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
