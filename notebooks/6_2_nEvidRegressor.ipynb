{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 6.2 Regressor for number of evidence to take (based on claim)\n","\n","- treat number of evidence to use for each claim as an ordinal categorical variable that can be regressed upon"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3998,"status":"ok","timestamp":1682171621643,"user":{"displayName":"Bug ADS","userId":"12220748713582006804"},"user_tz":-600},"id":"pBV7EEw8ws--","outputId":"a33bba8e-6a12-49d4-fa57-4097c6bfbd05"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IvlHUjGBxi6U"},"outputs":[],"source":["import json\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxJxwC3uxAHJ"},"outputs":[],"source":["with open ('./drive/My Drive/LAB/COMP90042 A3/data/raw/train-claims.json') as f:\n","    train_claims = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WnJs6bPCxdS-"},"outputs":[],"source":["with open ('./drive/My Drive/LAB/COMP90042 A3/data/raw/dev-claims.json') as f:\n","    dev_claims = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3gJUbs3YxsS2"},"outputs":[],"source":["with open ('./drive/My Drive/LAB/COMP90042 A3/data/raw/evidence.json') as f:\n","    evidence = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qu3MUOzgzok0"},"outputs":[],"source":["import random\n","random.seed(19260817)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FRx-o_iL3wkT"},"outputs":[],"source":["evid_id_list = [evid_id for evid_id in evidence]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6aGfiIv-ynQ3"},"outputs":[],"source":["training_data = []\n","\n","for id in train_claims:\n","\n","  claim_text = train_claims[id]['claim_text']\n","\n","  n_evid = len(train_claims[id]['evidences'])\n","\n","  training_data.append((claim_text, n_evid))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fjkog1CQz8IN"},"outputs":[],"source":["dev_data = []\n","\n","for id in dev_claims:\n","\n","  claim_text = dev_claims[id]['claim_text']\n","\n","  n_evid = len(dev_claims[id]['evidences'])\n","\n","  dev_data.append((claim_text, n_evid))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5950,"status":"ok","timestamp":1682171629396,"user":{"displayName":"Bug ADS","userId":"12220748713582006804"},"user_tz":-600},"id":"BA0eew_Zxs0O","outputId":"87f4f675-16c0-419d-813f-66566ddb0435"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.15.1+cu118)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n"]}],"source":["!pip install torch torchvision transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ACUDqscx48V"},"outputs":[],"source":["from transformers import BertModel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2063,"status":"ok","timestamp":1682171631451,"user":{"displayName":"Bug ADS","userId":"12220748713582006804"},"user_tz":-600},"id":"hFhfa0fVx-u1","outputId":"3c3b6a08-c962-4a29-89ff-ca954c314ddc"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["bert_model = BertModel.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jwvoS1RjyJw3"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsHw-MNu0yAs"},"outputs":[],"source":["# sentence1 = training_data[0][0][0]\n","# sentence2 = training_data[0][0][1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tjxlZNcK0zCQ"},"outputs":[],"source":["# tokens1 = tokenizer.tokenize(sentence1)\n","# tokens2 = tokenizer.tokenize(sentence2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxbE6S1A05cA"},"outputs":[],"source":["# tokens1 = ['[CLS]'] + tokens1 + ['[SEP]']\n","# tokens2 = tokens2 + ['[SEP]']\n","# tokens = tokens1 + tokens2\n","# print(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pzCn3EtP7lst"},"outputs":[],"source":["# len(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3m9nRV6S1HSD"},"outputs":[],"source":["# T = 512\n","\n","# padded_tokens = tokens + ['[PAD]' for _ in range(T-len(tokens))]\n","# print(padded_tokens)\n","\n","# attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n","# print(attn_mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3NNNw7a81e-5"},"outputs":[],"source":["# seg_ids = [0 for _ in range(len(tokens1))]\n","# seg_ids2 = [1 for _ in range(512-len(tokens1))]\n","# seg_ids.extend(seg_ids2)\n","# print(seg_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0E68ieNg12GZ"},"outputs":[],"source":["# token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n","# print(token_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nxpDJP6V2D2C"},"outputs":[],"source":["# import torch\n","\n","# token_ids_t = torch.tensor(token_ids).unsqueeze(0)\n","# attn_mask_t = torch.tensor(attn_mask).unsqueeze(0)\n","# seg_ids_t = torch.tensor(seg_ids).unsqueeze(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TOhJ_Rq2NTE"},"outputs":[],"source":["# outputs = bert_model(token_ids_t, attention_mask = attn_mask_t, token_type_ids = seg_ids_t, return_dict = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKFT82UO2T72"},"outputs":[],"source":["# hidden_reps = outputs.last_hidden_state"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdUwagsg2WAt"},"outputs":[],"source":["# print(hidden_reps[0,0,:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuIdphoS21tC"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from transformers import BertTokenizer\n","import pandas as pd\n","\n","class Dataset():\n","\n","    def __init__(self, data, maxlen):\n","\n","        #Store the contents of the file in a pandas dataframe\n","        self.data = data\n","\n","        #Initialize the BERT tokenizer\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","        self.maxlen = maxlen\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","\n","        #Selecting the sentence and label at the specified index in the data frame\n","        sentence = self.data[index][0]\n","        label = self.data[index][1]\n","\n","        #Preprocessing the text to be suitable for BERT\n","        tokens = tokenizer.tokenize(sentence)\n","        tokens = ['[CLS]'] + tokens + ['[SEP]'] #Insering the CLS and SEP token in the beginning and end of the sentence\n","        if len(tokens) < self.maxlen:\n","            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] #Padding sentences\n","        else:\n","            tokens = tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n","\n","        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n","        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n","\n","        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n","        attn_mask = (tokens_ids_tensor != 0).long()\n","  \n","        return tokens_ids_tensor, attn_mask, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qvOcemHR_bHt"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","#Creating instances of training and development set\n","#maxlen sets the maximum length a sentence can have\n","#any sentence longer than this length is truncated to the maxlen size\n","train_set = Dataset(training_data, maxlen = 512)\n","dev_set = Dataset(dev_data, maxlen = 512)\n","\n","#Creating intsances of training and development dataloaders\n","train_loader = DataLoader(train_set, batch_size = 16, num_workers = 2)\n","dev_loader = DataLoader(dev_set, batch_size = 16, num_workers = 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srZ-42Pz_oyO"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import BertModel\n","\n","class nEvidRegressor(nn.Module):\n","\n","    def __init__(self):\n","        super(nEvidRegressor, self).__init__()\n","        #Instantiating BERT model object \n","        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n","        \n","        #Classification layer\n","        #input dimension is 768 because [CLS] embedding has a dimension of 768\n","        #output dimension is 1 because we're working with a binary classification problem\n","        self.cls_layer = nn.Linear(768, 1)\n","\n","    def forward(self, seq, attn_masks):\n","        '''\n","        Inputs:\n","            -seq : Tensor of shape [B, T] containing token ids of sequences\n","            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n","        '''\n","\n","        #Feeding the input to BERT model to obtain contextualized representations\n","        outputs = self.bert_layer(seq, attention_mask = attn_masks, return_dict=True)\n","        cont_reps = outputs.last_hidden_state\n","\n","        #Obtaining the representation of [CLS] head (the first token)\n","        cls_rep = cont_reps[:, 0]\n","\n","        #Feeding cls_rep to the classifier layer\n","        prediction = self.cls_layer(cls_rep)\n","\n","        return prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":993,"status":"ok","timestamp":1682171633543,"user":{"displayName":"Bug ADS","userId":"12220748713582006804"},"user_tz":-600},"id":"fA0faPfGAeLl","outputId":"8534f32d-14c2-4c90-ff86-7067f77f6060"},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating the sentiment regressor, initialised with pretrained BERT-BASE parameters...\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Done creating the sentiment regressor.\n"]}],"source":["gpu = 0 #gpu ID\n","\n","print(\"Creating the sentiment regressor, initialised with pretrained BERT-BASE parameters...\")\n","net = nEvidRegressor()\n","net.cuda(gpu) #Enable gpu support for the model\n","print(\"Done creating the sentiment regressor.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGlc0FwVAlHB"},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","criterion = nn.MSELoss()\n","opti = optim.Adam(net.parameters(), lr = 2e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14254,"status":"ok","timestamp":1682172061200,"user":{"displayName":"Bug ADS","userId":"12220748713582006804"},"user_tz":-600},"id":"xgPD1lUmQDxJ","outputId":"7c220416-1596-40d5-d0a1-4c2edecf749c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (0.11.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.1)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.0.0+cu118)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.11.0)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"]}],"source":["!pip install torchmetrics\n","from torchmetrics import R2Score\n","r2score = R2Score().to(gpu)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L7RR7ohEAnjy"},"outputs":[],"source":["def get_accuracy_from_logits(logits, labels):\n","    probs = torch.sigmoid(logits.unsqueeze(-1))\n","    soft_probs = (probs > 0.5).long()\n","    acc = (soft_probs.squeeze() == labels).float().mean()\n","    return acc\n","    \n","\n","def evaluate(net, criterion, dataloader, gpu):\n","    net.eval()\n","\n","    mean_acc, mean_loss = 0, 0\n","    count = 0\n","\n","    with torch.no_grad():\n","        for seq, attn_masks, labels in dataloader:\n","            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n","            prediction = net(seq, attn_masks)\n","            mean_loss += criterion(prediction.squeeze(-1), labels.float()).item()\n","            count += 1\n","\n","    return r2score(prediction.squeeze(-1), labels), mean_loss / count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eBze9eaUAqLg"},"outputs":[],"source":["import time\n","\n","def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n","\n","    best_acc = 0\n","    st = time.time()\n","    for ep in range(max_eps):\n","        \n","        net.train()\n","        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n","            #Clear gradients\n","            opti.zero_grad()  \n","            #Converting these to cuda tensors\n","            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n","\n","            #Obtaining the logits from the model\n","            prediction = net(seq, attn_masks)\n","\n","            #Computing loss\n","            loss = criterion(prediction.squeeze(-1), labels.float())\n","\n","            #Backpropagating the gradients\n","            loss.backward()\n","\n","            #Optimization step\n","            opti.step()\n","              \n","            if it % 100 == 0:\n","                \n","                r2 = r2score(prediction.squeeze(-1), labels)\n","                # acc = get_accuracy_from_logits(logits, labels)\n","                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), r2, (time.time()-st)))\n","                st = time.time()\n","\n","        \n","        dev_acc, dev_loss = evaluate(net, criterion, dev_loader, gpu)\n","        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n","        if dev_acc > best_acc:\n","            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n","            best_acc = dev_acc\n","            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":335810,"status":"ok","timestamp":1682172768397,"user":{"displayName":"Bug ADS","userId":"12220748713582006804"},"user_tz":-600},"id":"h5mhGZTSBB2S","outputId":"14681a04-2a52-43cd-c779-a963d93bb0f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 0 of epoch 0 complete. Loss: 1.4166837930679321; Accuracy: 0.5118828415870667; Time taken (s): 1.5630509853363037\n","Epoch 0 complete! Development Accuracy: -0.26622307300567627; Development Loss: 2.9280277967453\n","Iteration 0 of epoch 1 complete. Loss: 0.49700045585632324; Accuracy: 0.828758955001831; Time taken (s): 111.84511637687683\n","Epoch 1 complete! Development Accuracy: -0.4728083610534668; Development Loss: 3.0915986776351927\n","Iteration 0 of epoch 2 complete. Loss: 0.4696519374847412; Accuracy: 0.8381818532943726; Time taken (s): 111.70906257629395\n","Epoch 2 complete! Development Accuracy: -0.7219902276992798; Development Loss: 3.4682044982910156\n"]}],"source":["num_epoch = 3\n","\n","#fine-tune the model\n","train(net, criterion, opti, train_loader, dev_loader, num_epoch, gpu)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jomEBD_eOi8U"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
